# MedCMU-HPC Comprehensive Reference

> This document provides ALL information about the MedCMU High-Performance Computing (HPC) system. Designed for AI assistants and users who need quick reference.

---

## System Identity

- **Name**: MedCMU-HPC (Faculty of Medicine High-Performance Computing System)
- **Organization**: Faculty of Medicine, Chiang Mai University, Thailand
- **Purpose**: Research computing for medical and health sciences
- **Login Host**: `raptor.med.cmu.ac.th`
- **OnDemand Portal**: `https://raptor.med.cmu.ac.th:9000`

---

## Getting Started

### Registration
1. **User Registration**: https://cmu.to/medcmu-hpc-user-register
2. **Project Registration**: https://cmu.to/medcmu-hpc-proj-register
3. Response within **5 business days**

### Support Contacts
| Channel | Contact |
|---------|---------|
| Email | supporthpc-med@cmu.ac.th |
| Consultation | https://cal.com/nattawet-sri/30min |
| Documentation | https://cmu.to/medcmu-hpc |
| Change Request | https://cmu.to/medcmu-hpc-req |

---

## Connecting to the Cluster

### SSH Access
```bash
ssh <username>@raptor.med.cmu.ac.th
```

### Windows Users
Recommended: **MobaXTerm** (https://mobaxterm.mobatek.net/download.html)

### Passwordless Login (SSH Keys)
```bash
# Generate key
ssh-keygen -t ed25519 -C "your_email"

# Copy to server
ssh-copy-id -i ~/.ssh/id_ed25519.pub <username>@raptor.med.cmu.ac.th
```

### File Transfer
```bash
# Upload
scp file.txt <username>@raptor.med.cmu.ac.th:/project/<ProjectID>/

# Download
scp <username>@raptor.med.cmu.ac.th:/project/<ProjectID>/file.txt .

# Rsync (recommended for large transfers)
rsync -avz --progress local_folder/ <username>@raptor.med.cmu.ac.th:/project/<ProjectID>/
```

---

## Cluster Architecture

### Nodes
| Node Type | Hostname | Purpose |
|-----------|----------|---------|
| Login | raptor | User access, job submission |
| CPU/Memory | raptor-m-001 to raptor-m-004 | General compute |
| GPU | raptor-g-001, raptor-g-002 | GPU-accelerated workloads |

### SLURM Partitions
| Partition | Max Time | Use Case |
|-----------|----------|----------|
| short | 1 hour | Testing, quick jobs |
| medium | 12 hours | Standard workloads |
| long | 7 days | Extended jobs |
| gpu | 7 days | GPU workloads |
| compute | 7 days | General compute |

---

## Storage System

| Location | Purpose | Quota | Backup |
|----------|---------|-------|--------|
| `/home/<username>` | Personal files | 100 GB | Yes |
| `/project/<ProjectID>` | Project data | 1 TB | Yes |
| `/scratch/<ProjectID>` | Temp work | 1 TB | No |

### Check Quota
```bash
myquota
```

### Data Retention
- **Project data**: Accessible 60 days after project ends
- **User home**: Accessible 1 year after project ends
- After these periods, data is **permanently deleted**

---

## SLURM Job Scheduler

### Key Concepts
- **Never run programs on login node** - use SLURM
- Jobs can be **interactive** (`srun`) or **batch** (`sbatch`)

### Essential Commands
| Command | Purpose |
|---------|---------|
| `sbatch script.sh` | Submit batch job |
| `squeue` | View job queue |
| `squeue -u $USER` | View your jobs |
| `scancel <jobid>` | Cancel job |
| `sinfo` | View node status |
| `sbalance` | Check compute credits |
| `sacct` | View job history |

### Interactive Session
```bash
# Basic shell
srun --pty bash

# Python with resources
module load python/3.12.7
srun -p short -t 1:00:00 --mem=4G -c 1 --pty python

# GPU session
srun -p gpu --gpus=1 -t 2:00:00 --mem=16G -c 4 --pty bash
```

### Batch Job Script Template
```bash
#!/bin/bash
#SBATCH --job-name=myjob
#SBATCH --partition=medium
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --account=<ProjectID>
#SBATCH --output=output_%j.log
#SBATCH --error=error_%j.log

module load python/3.12.7
python myscript.py
```

### Common SLURM Options
| Option | Description |
|--------|-------------|
| `--partition/-p` | Partition name |
| `--time/-t` | Time limit (dd-hh:mm:ss) |
| `--mem` | Memory per node |
| `--cpus-per-task/-c` | CPU cores per task |
| `--gpus/-G` | Number of GPUs |
| `--account/-A` | Project account |
| `--output/-o` | Output file |
| `--error/-e` | Error file |

### Job Status Codes
| Code | Meaning |
|------|---------|
| PD | Pending |
| R | Running |
| CG | Completing |
| CD | Completed |
| F | Failed |
| TO | Timeout |

---

## Software Environment Modules

### Commands
```bash
module avail              # List all available software
module spider <name>      # Search for software
module load <software>    # Load software
module list               # Show loaded modules
module purge              # Unload all modules
```

### Common Modules
```bash
module load python/3.12.7
module load R/4.3.0
module load cuda/12.0
module load apptainer
module load mamba/1.5.8
```

---

## Apptainer Containers

### Available Container Images
Location: `/common/sif/`

| Software | Path |
|----------|------|
| AlphaFold 2 | `/common/sif/alphafold2/2.0.0.sif` |
| AlphaFold 2 Multimer | `/common/sif/alphafold2-multimer/1.0.0.sif` |
| AutoDock GPU | `/common/sif/autodock-gpu/2020.06.sif` |
| Clara Parabricks | `/common/sif/clara-parabricks/4.4.0.sif` |
| DiffDock | `/common/sif/diffdock/2.0.0.sif` |
| GROMACS | `/common/sif/gromacs/2023.2.sif` |
| MONAI | `/common/sif/monai/1.4.0.sif` |
| TensorFlow | `/common/sif/tensorflow/24.10-tf2-py3.sif` |

### Container Commands
```bash
# List all containers
ls /common/sif/*/*.sif

# Run interactively
apptainer shell image.sif

# Execute command
apptainer exec image.sif python script.py

# With GPU support
apptainer exec --nv image.sif python script.py

# Pull from Docker Hub
apptainer pull myimage.sif docker://repository:tag
```

### Container SLURM Script
```bash
#!/bin/bash
#SBATCH --job-name=container_job
#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --time=02:00:00
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G

module load apptainer
apptainer exec --nv /common/sif/tensorflow/24.10-tf2-py3.sif python train.py
```

---

## OnDemand Web Portal

**URL**: https://raptor.med.cmu.ac.th:9000

### Features
- Web-based file browser
- Interactive apps (RStudio, Jupyter)
- Job management interface

### Launching Interactive Apps
1. Go to **Interactive Apps** menu
2. Select application (RStudio/Jupyter)
3. Configure resources:
   - Project Account: `a26XXXX`
   - Partition: `short`/`compute`/`gpu`
   - Hours, CPUs, Memory, GPUs
4. Click **Launch**
5. Wait for status "Running", then click **Connect**

---

## Running Jupyter with SLURM

### Script Template
```bash
#!/bin/bash
#SBATCH --job-name=jupyter
#SBATCH --partition=compute
#SBATCH --time=04:00:00
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G

module load mamba/1.5.8
eval "$(micromamba shell hook --shell bash)"
micromamba activate jupyter

port=$(shuf -i 6000-9999 -n 1)
node=$(hostname -s)

echo "SSH tunnel: ssh -L $port:$node:$port $USER@raptor.med.cmu.ac.th"
echo "URL: http://localhost:$port"

jupyter lab --no-browser --port $port --ip=$node
```

### Access
1. Submit job: `sbatch jupyter.slurm`
2. Check logs for port and token
3. SSH tunnel: `ssh -L <port>:<node>:<port> user@raptor.med.cmu.ac.th`
4. Open: `http://localhost:<port>/?token=<token>`

---

## Python Virtual Environments

```bash
# Create virtual environment
python -m venv ~/myenv

# Activate
source ~/myenv/bin/activate

# Install packages
pip install numpy pandas scikit-learn

# Deactivate
deactivate
```

---

## User/Project Management (Admin)

### Create User
```bash
./createUser.sh -u username -n FirstName -l LastName -o OrganizationName
sss_cache -E
```

### Create Project
```bash
./createProject.sh -n ProjectShortName -i a26XXXXX -g GIDCODE -u user1,user2 -o OrgName -d 'Description'
sss_cache -E
```

### Add Users to Project
```bash
./addProjectUser.sh -i a26XXXXX -u user1,user2
```

### Project ID Format
- `a26XXXX_ShortName` - AI project (2026)
- `o26XXXX_ShortName` - Omics project
- `w26XXXX_ShortName` - Workshop project

---

## Troubleshooting

### Job Diagnostics
```bash
sacct --jobs=<jobid> --format=jobname,nnodes,ncpus,maxrss,elapsed
scontrol show jobid -dd <jobid>
```

### Node Status
```bash
sinfo -R                    # Drained nodes
scontrol show node <name>   # Node details
sdiag                       # Scheduler diagnostics
```

### Common Pending Reasons
| Reason | Solution |
|--------|----------|
| `(Resources)` | Reduce request or wait |
| `(Priority)` | Wait for other jobs |
| `(AssocGrpBillingMinutes)` | Request more credits |
| `(QOSMaxJobsPerUserLimit)` | Wait for jobs to finish |

### Check Logs
```bash
cat slurm-<jobid>.out
cat slurm-<jobid>.err
```

---

## SOP Documents

### Admin SOPs
| Code | Title |
|------|-------|
| SOP-HPC-A001 | User Onboarding |
| SOP-HPC-A002 | Project Approval |
| SOP-HPC-A003 | Resource Allocation |
| SOP-HPC-A004 | System Maintenance |
| SOP-HPC-A005 | Incident Response |
| SOP-HPC-A006 | Software Installation |

### User SOPs
| Code | Title |
|------|-------|
| SOP-HPC-U001 | Account Registration |
| SOP-HPC-U002 | Project Registration |
| SOP-HPC-U003 | Request Resources |
| SOP-HPC-U004 | Data Management |
| SOP-HPC-U005 | Troubleshooting |

---

## Key Policies

1. **Account Security**: Keep credentials confidential, never share
2. **Login Node**: For job management only, not computation
3. **Resource Limits**: Max ~4 concurrent jobs per user
4. **Acceptable Use**: Research purposes only, comply with Thai law
5. **Data**: Research data only, no personal/illegal content
6. **Acknowledgment**: Cite MedCMU-HPC in publications

---

## Quick Reference

```bash
# Connect
ssh user@raptor.med.cmu.ac.th

# Check resources
myquota                     # Storage
sbalance                    # Compute credits

# Jobs
sbatch script.sh            # Submit
squeue -u $USER             # Status
scancel <id>                # Cancel

# Software
module avail                # List
module load <name>          # Load

# Containers
apptainer exec --nv <sif> cmd

# Transfer
rsync -avz src/ user@raptor.med.cmu.ac.th:/project/<id>/
```

---

*MedCMU-HPC Reference v2.0 | Last Updated: 2026-01-18*
